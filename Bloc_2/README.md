# üì¶ Certification DE ‚Äì Bloc 2
## Project - Amazon Review Analysis
## üöÄ Pipeline ETL
---

Pipeline ETL automatis√© pour extraire, transformer et charger les donn√©es d'avis Amazon depuis PostgreSQL vers S3, Snowflake et MongoDB.

## D√©marrage Rapide

### 1Ô∏è‚É£ D√©marrage de l'infrastructure Docker

```bash
# 1. PostgreSQL (contient les donn√©es source - initialisation automatique)
docker-compose -f docker-compose.postgres.yml up -d

# 2. MongoDB (stocke les logs et donn√©es rejet√©es)
docker-compose -f docker-compose.mongodb.yml up -d

# 3. Airflow (orchestration du pipeline ETL)
docker-compose -f docker-compose.airflow.yml up -d
```

### 2Ô∏è‚É£ Configuration des credentials

**Cr√©er le fichier `.env` √† la racine:**
```bash
cp .env.example .env
# √âditer .env avec vos credentials AWS et Snowflake
```

**Variables obligatoires dans `.env`:**
```bash
# AWS S3
AWS_ACCESS_KEY_ID=votre_access_key
AWS_SECRET_ACCESS_KEY=votre_secret_key
AWS_S3_BUCKET=votre-bucket
AWS_REGION=eu-west-1

# Snowflake
SNOWFLAKE_USER=votre_user
SNOWFLAKE_PASSWORD=votre_password
SNOWFLAKE_ACCOUNT=votre_account
SNOWFLAKE_DATABASE=AMAZON_REVIEWS
SNOWFLAKE_SCHEMA=ANALYTICS
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_ROLE=ACCOUNTADMIN

# PostgreSQL et MongoDB sont d√©j√† configur√©s pour Docker
```

### 3Ô∏è‚É£ D√©clencher le pipeline complet
```bash
docker exec airflow-webserver airflow dags trigger main_orchestrator
```

**Le pipeline ex√©cutera automatiquement:**
1. ‚úÖ Initialisation MongoDB (collections + indexes)
2. ‚úÖ Initialisation Snowflake (database + schema + tables)
3. ‚úÖ Extraction PostgreSQL ‚Üí S3 (8 tables, anonymisation buyer_id)
4. ‚úÖ Transformation et chargement ‚Üí Snowflake + MongoDB

**Monitoring:**
- **Interface Airflow**: http://localhost:8080 (login: admin / password: admin)
- **MongoDB UI**: http://localhost:8081
- **Logs en temps r√©el**: `docker logs -f airflow-scheduler`

---

## Commandes Utiles

```bash
# Arr√™ter tous les services
docker-compose -f docker-compose.airflow.yml down
docker-compose -f docker-compose.mongodb.yml down
docker-compose -f docker-compose.postgres.yml down

# Red√©marrer un service sp√©cifique
docker-compose -f docker-compose.airflow.yml restart

# R√©initialiser PostgreSQL (supprime les donn√©es)
docker-compose -f docker-compose.postgres.yml down -v
docker-compose -f docker-compose.postgres.yml up -d
```

## Tests de Qualit√©

Le projet inclut une suite compl√®te de tests de qualit√© des donn√©es :

```bash
cd src_code

# Ex√©cuter les tests
python tests/test_data_quality.py

# G√©n√©rer le rapport HTML
python scripts/generate_quality_report.py
```

**8 tests automatis√©s** :
- Connexion PostgreSQL
- Validation des ratings (1-5)
- D√©tection des doublons
- Champs obligatoires non-NULL
- Prix positifs
- Textes non-vides
- Coh√©rence des types
- Int√©grit√© r√©f√©rentielle

Les rapports sont disponibles dans `src_code/reports/` (JSON + HTML).

## Technologies

- **Orchestration** : Apache Airflow 2.8.3 (Docker)
- **Source** : PostgreSQL 17 (Docker)
- **Data Lake** : AWS S3
- **Data Warehouse** : Snowflake
- **Logs & Rejets** : MongoDB 7.0 (Docker)
- **ETL** : Python 3.11+ avec pandas, boto3, snowflake-connector
- **Conteneurisation** : Docker & Docker Compose
